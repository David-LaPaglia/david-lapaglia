<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="style.css" rel="stylesheet">
    <title>David LaPaglia - Neural Networks in Text Mining</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <style>
        /* Neural Networks specific styling */
        .insights-container {
            background-color: rgba(185, 202, 200, 0.1);
            border-radius: 8px;
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-left: 4px solid var(--primary-color);
        }
        
        .insights-container h4 {
            color: var(--primary-color);
            margin-top: 1.5rem;
            border-bottom: 1px solid rgba(29, 89, 82, 0.2);
            padding-bottom: 0.5rem;
        }
        
        .word-analysis {
            margin: 1.5rem 0;
            text-align: center;
        }
        
        .word-analysis img {
            max-width: 100%;
            border-radius: 8px;
            box-shadow: 0 4px 12px rgba(0,0,0,0.15);
        }
        .classification-report {
            margin: 1.5rem 0;
        }
        
        .report-table {
            background-color: #1a1a2e;
            color: #991e1e;
            font-family: monospace;
            padding: 1.2rem;
            border-radius: 6px;
            overflow-x: auto;
            line-height: 1.5;
            white-space: pre;
        }
        
        .model-visual {
            border-radius: 8px;
            box-shadow: 0 4px 12px rgba(0,0,0,0.15);
            transition: transform 0.3s ease;
        }
        
        .model-visual:hover {
            transform: scale(1.02);
        }
        
        .visualization-container {
            text-align: center;
            margin: 1rem 0 2rem 0;
        }
        .results-summary {
            background-color: 991e1e;
            border-radius: 8px;
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-left: 4px solid var(--primary-color);
        }
        
        .results-summary ul {
            margin: 1rem 0;
            padding-left: 1.5rem;
        }
        
        .results-summary li {
            margin-bottom: 0.5rem;
        }
        .model-table {
            width: 100%;
            border-collapse: collapse;
            margin: 1rem 0;
            font-family: monospace;
            background-color: #1a1a2e;
            color: #fff;
            border-radius: 6px;
            overflow: hidden;
        }
        
        .model-table th {
            background-color: #2a2a45;
            padding: 10px;
            text-align: left;
            font-weight: bold;
        }
        
        .model-table td {
            padding: 8px 10px;
            border-top: 1px solid #3a3a55;
        }
        
        .model-table tr:hover {
            background-color: #252540;
        }
        
        .layer-type {
            color: #64b5f6;
        }
        
        .shape-none {
            color: #64b5f6;
        }
        
        .shape-dim {
            color: #81c784;
        }
        
        .param-count {
            color: #81c784;
            font-weight: bold;
        }
        
        .model-params {
            background-color: #1a1a2e;
            color: #fff;
            padding: 1rem;
            font-family: monospace;
            border-radius: 0 0 6px 6px;
            margin-top: -1rem;
        }
        
        .results-log {
            background-color: #1a1a2e;
            color: #ddd;
            font-family: monospace;
            line-height: 1.6;
        }
        
        .training-results {
            margin-top: 1rem;
        }
        .step-header {
            font-size: 1.2rem;
            color: var(--primary-color);
            margin-bottom: 1rem;
            display: block;
        }
        
        .data-prep-steps {
            counter-reset: step-counter;
            list-style-type: none;
            padding-left: 0;
        }
        
        .data-prep-steps > li {
            position: relative;
            background-color: rgba(185, 202, 200, 0.1);
            border-radius: 8px;
            padding: 1.5rem;
            margin-bottom: 2rem;
            box-shadow: 0 2px 5px rgba(0,0,0,0.05);
        }
        
        .data-prep-steps > li::before {
            content: counter(step-counter);
            counter-increment: step-counter;
            position: absolute;
            left: -30px;
            top: 50%;
            transform: translateY(-50%);
            background-color: var(--primary-color);
            color: white;
            width: 36px;
            height: 36px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: bold;
        }
        
        .indented-list {
            margin-left: 2rem;
            border-left: 3px solid rgba(29, 89, 82, 0.2);
            padding-left: 1.5rem;
            margin-bottom: 1.5rem;
            list-style-type: square;
        }
        
        .indented-list li {
            margin-bottom: 0.8rem;
        }
        .project-overview {
            background-color: var(--secondary-color);
            padding: 2rem;
            border-radius: 8px;
            margin-bottom: 2rem;
            color: var(--primary-color);
        }
        
        .implementation {
            background: white;
            border-radius: 8px;
            padding: 2rem;
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
            margin-bottom: 2rem;
        }
        
        .implementation h4 {
            color: var(--primary-color);
            border-bottom: 2px solid var(--secondary-color);
            padding-bottom: 0.5rem;
            margin: 1.5rem 0 1rem 0;
        }
        
        .implementation ol {
            padding-left: 1.5rem;
        }
        
        .implementation ol > li {
            margin-bottom: 1.5rem;
        }
        
        .implementation ul {
            margin-top: 0.5rem;
        }
        
        .implementation li {
            margin-bottom: 1rem;
            line-height: 1.6;
            padding-left: 0.5rem;
        }
        
        .implementation ul {
            list-style-type: circle;
            padding-left: 2rem;
            margin-bottom: 1.5rem;
        }
        
        .implementation ul ul {
            list-style-type: square;
            margin-top: 1rem;
            margin-bottom: 1rem;
        }
        
        .implementation ol > li {
            margin-bottom: 2rem;
            padding-bottom: 1rem;
            border-bottom: 1px dashed var(--secondary-color);
        }
        
        .implementation ol > li:last-child {
            border-bottom: none;
        }
        
        .implementation ul > li::marker {
            color: var(--primary-color);
        }
        
        .neural-architectures {
            background: white;
            border-radius: 8px;
            padding: 2rem;
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
            margin-bottom: 2rem;
        }
        
        .architecture-card {
            border: 1px solid var(--secondary-color);
            border-radius: 8px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            background-color: rgba(185, 202, 200, 0.1);
        }
        
        .code-snippet {
            background-color: #f8f8f8;
            border-radius: 6px;
            padding: 1rem;
            overflow-x: auto;
            font-family: monospace;
            font-size: 0.9rem;
            line-height: 1.5;
            border-left: 4px solid var(--primary-color);
        }
        
        .code-example {
            margin-top: 2rem;
        }
        
        code {
            background-color: #f0f0f0;
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-family: monospace;
            font-size: 0.9em;
        }
    </style>
</head>
<body>
    <nav id="nav">
        <ul>
            <li><a href="index.html">Home</a></li>
            <li><a href="about.html">About Me</a></li>
            <li class="dropdown">
                <a href="projects.html">Projects</a>
                <ul class="dropdown-content">
                    <li><a href="projects.html">All Projects</a></li>
                    <li class="dropdown">
                        <a href="textmining.html">Text Mining Projects</a>
                        <ul class="dropdown-content secondary-dropdown">
                            <li class="dropdown">
                                <a href="sustainabilityanalysis.html">Sustainability Analysis</a>
                                <ul class="dropdown-content tertiary-dropdown">
                                    <li><a href="sustain_intro.html">Introduction</a></li>
                                    <li><a href="sustain_data.html">Data</a></li>
                                </ul>
                            </li>
                            <li><a href="neural_networks.html">Neural Networks</a></li>
                        </ul>
                    </li>
                </ul>
            </li>
            <li><a href="resume.html">My Resumé</a></li>
        </ul>
    </nav>

    <div class="content">
        <h2 style="color: var(--primary-color); text-align: center; margin-bottom: 1.5rem;">Neural Networks in Text Mining</h2>
        
        <section class="project-overview">
            <h3><i class="fas fa-brain"></i> Overview: Neural Networks for NLP</h3>
            <p>A feed forward artificial neural network (ANN) designed to predict sentiment analysis in climate change discourse</p>
        </section>
        <section class="project-overview">
        <img src="https://www.investopedia.com/thmb/5-hnhHpOzLM2GVXPlSstg8tJYLw=/1500x0/filters:no_upscale():max_bytes(150000):strip_icc()/dotdash_Final_Neural_Network_Apr_2020-01-5f4088dfda4c49d99a4d927c9a3a5ba0.jpg" alt="Feed Forward Neural Network" class="project-image model-visual" style="max-width: 500px; display: block; margin: 1rem auto;">
        <p>Here is a link to the <a href="https://en.wikipedia.org/wiki/Feedforward_neural_network">Wikipedia page</a> for more information on neural networks.</p>
        </section>

        <section class="implementation">
            <h3><i class="fas fa-cogs"></i> Implementation Details</h3>
            
            <h4><i class="fas fa-database"></i> Data Preparation</h4>
            <p>The text data preparation involved a comprehensive multi-stage process to ensure high-quality inputs for the neural network model:</p>
            
            <ol class="data-prep-steps">
                <li>
                    <div class="step-header"><strong>Data Collection and Loading</strong></div>
                    <ul>
                        <li>Loaded data from multiple JSON files containing Reddit comments and news articles</li>
                        <li>Used custom functions (<code>load_reddit_comments</code> and <code>load_news_articles</code>) to parse these files</li>
                        <li>Created a combined corpus by concatenating data from both sources with source labels</li>
                        <li>Initial dataset had 8,448 documents with a text field and source field (Reddit vs news)</li>
                    </ul>
                </li>
                
                <li>
                    <div class="step-header"><strong>Automatic Labeling</strong></div>
                    <ul>
                        <li>Implemented a lexicon-based approach using keyword matching</li>
                        <li>Created two sets of keywords:</li>
                    </ul>
                    <ul class="indented-list">
                        <li>Positive keywords: climate, global warming, renewable, carbon neutral, sustainability, etc.</li>
                        <li>Negative keywords: climate hoax, fake science, scam, not real, climate agenda, etc.</li>
                    </ul>
                    <ul>
                        <li>Created a robust labeling function (<code>very_robust_auto_label</code>) that:</li>
                    </ul>
                    <ul class="indented-list">
                        <li>Counted occurrences of positive and negative keywords in each text</li>
                        <li>Assigned label 1 (positive) if more positive keywords than negative</li>
                        <li>Assigned label 0 (negative) if more negative keywords than positive</li>
                        <li>Assigned label -1 (neutral/unclear) if equal or no matches</li>
                    </ul>
                    <ul>
                        <li>Initially labeled data was highly imbalanced:</li>
                    </ul>
                    <ul class="indented-list">
                        <li>7,624 neutral/unclear (-1)</li>
                        <li>763 positive (1)</li>
                        <li>61 negative (0)</li>
                    </ul>
                </li>
                
                <li>
                    <div class="step-header"><strong>Dataset Filtering and Balancing</strong></div>
                    <ul>
                        <li>Filtered out neutral/unclear documents (label -1)</li>
                        <li>Resulting in 824 labeled documents (763 positive, 61 negative)</li>
                        <li>Used scikit-learn's <code>resample</code> to balance the dataset:</li>
                    </ul>
                    <ul class="indented-list">
                        <li>Identified majority class (positive) and minority class (negative)</li>
                        <li>Upsampled the minority class with replacement to match majority class size</li>
                        <li>Combined original majority samples with upsampled minority samples</li>
                        <li>Final balanced dataset contained 763 samples of each class (1,526 total)</li>
                    </ul>
                    <ul>
                        <li>Note on upsampling with replacement: Each of the 61 negative samples appears approximately 12-13 times in the final balanced dataset</li>
                    </ul>
                </li>
                
                <li>
                    <div class="step-header"><strong>Feature Engineering</strong></div>
                    <ul>
                        <li>Used CountVectorizer for text representation:</li>
                    </ul>
                    <ul class="indented-list">
                        <li>Removed English stop words</li>
                        <li>Set maximum features to 10,000</li>
                        <li>Converted to lowercase</li>
                    </ul>
                    <ul>
                        <li>Transformed text into a sparse matrix of token counts</li>
                        <li>Resulting feature matrix had shape (824, 6392) before balancing</li>
                        <li>After balancing and applying CountVectorizer, produced properly balanced input features</li>
                    </ul>
                </li>
                
                <li>
                    <div class="step-header"><strong>Train-Test Split</strong></div>
                    <ul>
                        <li>Split the balanced dataset into training and testing sets (80%/20%)</li>
                        <li>Used stratified sampling to maintain class balance in both sets</li>
                    </ul>
                </li>
            </ol>
        </section>
        
        <section class="neural-architectures">
            <h3><i class="fas fa-network-wired"></i> Neural Network Architectures</h3>
            
            <div class="architecture-grid">
                <!-- Architecture 1 -->
                <div class="architecture-card">
                    <h4><i class="fas fa-project-diagram"></i> Feed-Forward Deep Neural Network</h4>
                    <p>A multi-layer feed-forward neural network designed for sentiment analysis of climate-related text. This architecture processes text data after vectorization to classify sentiment as positive or negative.</p>
               
                    <h5>Key Components:</h5>
                    <ul>
                        <li>An input layer with 6392 features (from your CountVectorizer)</li>
                        <li>A dense hidden layer with 128 neurons followed by dropout</li>
                        <li>A second dense hidden layer with 64 neurons followed by dropout</li>
                        <li>An output layer with a single neuron (for binary classification)</li>
                    </ul>
                    
                    <h5>Model Summary:</h5>
                    <div class="model-summary">
                        <table class="model-table">
                            <thead>
                                <tr>
                                    <th>Layer (type)</th>
                                    <th>Output Shape</th>
                                    <th>Param #</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>dense (<span class="layer-type">Dense</span>)</td>
                                    <td>(<span class="shape-none">None</span>, <span class="shape-dim">128</span>)</td>
                                    <td><span class="param-count">818,304</span></td>
                                </tr>
                                <tr>
                                    <td>dropout (<span class="layer-type">Dropout</span>)</td>
                                    <td>(<span class="shape-none">None</span>, <span class="shape-dim">128</span>)</td>
                                    <td><span class="param-count">0</span></td>
                                </tr>
                                <tr>
                                    <td>dense_1 (<span class="layer-type">Dense</span>)</td>
                                    <td>(<span class="shape-none">None</span>, <span class="shape-dim">64</span>)</td>
                                    <td><span class="param-count">8,256</span></td>
                                </tr>
                                <tr>
                                    <td>dropout_1 (<span class="layer-type">Dropout</span>)</td>
                                    <td>(<span class="shape-none">None</span>, <span class="shape-dim">64</span>)</td>
                                    <td><span class="param-count">0</span></td>
                                </tr>
                                <tr>
                                    <td>dense_2 (<span class="layer-type">Dense</span>)</td>
                                    <td>(<span class="shape-none">None</span>, <span class="shape-dim">1</span>)</td>
                                    <td><span class="param-count">65</span></td>
                                </tr>
                            </tbody>
                        </table>
                        <div class="model-params">
                            <p><strong>Total params:</strong> <span class="param-count">826,625</span> (3.15 MB)</p>
                            <p><strong>Trainable params:</strong> <span class="param-count">826,625</span> (3.15 MB)</p>
                            <p><strong>Non-trainable params:</strong> <span class="param-count">0</span> (0.00 B)</p>
                        </div>
                    </div>
                    <pre class="code-snippet">
# Model architecture implementation
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

def create_model(input_dim=6392):
    model = Sequential([
        Dense(128, activation='relu', input_dim=input_dim),
        Dropout(0.2),
        Dense(64, activation='relu'),
        Dropout(0.2),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam',
                  loss='binary_crossentropy',
                  metrics=['accuracy'])
    return model
                    </pre>
                </div>
            </div>
        </section>
            
            <h4><i class="fas fa-code"></i> Model Development</h4>
            <p>This is a classic deep learning approach for text classification, with the following key characteristics:</p>
            <ul>
                <li>Text is first transformed into numerical features using the Bag-of-Words approach (CountVectorizer)</li>
                <li>The neural network then learns patterns in these feature vectors</li>
                <li>The output is a binary prediction (positive/negative sentiment)</li>
                <li>The model is used to classify text related to climate/environmental topics, with balanced classes for equal representation of positive and negative sentiment examples</li>
            </ul>
            
            <div class="code-example">
                <h4><i class="fas fa-cog fa-spin"></i> Training Process</h4>
                <pre class="code-snippet">
# Training process for the model
def train_model(model, X_train, y_train, X_val, y_val):
    history = model.fit(
        X_train, y_train,
        epochs=10,
        batch_size=32,
        validation_data=(X_val, y_val),
        callbacks=[
            tf.keras.callbacks.EarlyStopping(
                monitor='val_loss', 
                patience=3, 
                restore_best_weights=True
            )
        ]
    )
    return history, model
                </pre>
                
                <h5>Training Results:</h5>
                <div class="training-results">
                    <pre class="code-snippet report-table">
Epoch 1/5
39/39 ━━━━━━━━━━━━━━━━━━━━ 1s 10ms/step - accuracy: 0.6128 - loss: 0.6437 - val_accuracy: 1.0000 - val_loss: 0.3749
Epoch 2/5
39/39 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.9896 - loss: 0.2916 - val_accuracy: 0.9967 - val_loss: 0.0542
Epoch 3/5
39/39 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 1.0000 - loss: 0.0373 - val_accuracy: 0.9967 - val_loss: 0.0167
Epoch 4/5
39/39 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 1.0000 - loss: 0.0090 - val_accuracy: 0.9967 - val_loss: 0.0118
Epoch 5/5
39/39 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 1.0000 - loss: 0.0044 - val_accuracy: 0.9967 - val_loss: 0.0119</pre>
                </div>
            </div>
        </section>
        
        <section class="results">
            <h3><i class="fas fa-chart-line"></i> Model Performance & Evaluation</h3>
            
            <div class="results-summary">
                <p>The neural network achieved excellent performance on the sentiment classification task:</p>
                <ul>
                    <li><strong>Training accuracy:</strong> 100% by the third epoch</li>
                    <li><strong>Validation accuracy:</strong> 99.67%</li>
                    <li><strong>Training loss:</strong> Reduced from 0.6437 to 0.0044</li>
                    <li><strong>Validation loss:</strong> Stabilized around 0.01</li>
                </ul>
                
                <p>The model shows strong convergence with minimal signs of overfitting, as indicated by the close alignment between training and validation metrics after epoch 2.</p>
                
                <h4><i class="fas fa-table"></i> Confusion Matrix</h4>
                <div class="visualization-container">
                    <img src="images/ann/output.png" alt="Confusion Matrix" class="project-image model-visual" style="max-width: 500px; display: block; margin: 1rem auto;">
                </div>
                
                <h4><i class="fas fa-file-alt"></i> Classification Report</h4>
                <div class="classification-report">
                    <pre class="code-snippet report-table">
              precision    recall  f1-score   support

    Negative       0.99      1.00      1.00       153
    Positive       1.00      0.99      1.00       153

    accuracy                           1.00       306
   macro avg       1.00      1.00      1.00       306
weighted avg       1.00      1.00      1.00       306
                    </pre>
                </div>
                
            </div>
        </section>
        
        <section class="insights">
            <h3><i class="fas fa-lightbulb"></i> Insights & Conclusions</h3>
            
            <div class="insights-container">
                <p>The neural network sentiment analysis model revealed several interesting patterns in climate change discourse:</p>
                
                <h4><i class="fas fa-chart-bar"></i> Key Word Analysis</h4>
                <p>By examining the most frequent words in each sentiment class, we gained valuable insights into the language patterns associated with positive and negative sentiment in climate discourse:</p>
                
                <div class="word-analysis">
                    <img src="images/ann/output1.png" alt="Word Analysis" class="project-image model-visual" style="max-width: 500px; display: block; margin: 1rem auto;">
                </div>
                <div class="word-analysis">
                    <img src="images/ann/Screenshot 2025-04-29 at 10.06.22 PM.png" alt="Word Analysis" class="project-image model-visual" style="max-width: 500px; display: block; margin: 1rem auto;">
                </div>
                
                <h4><i class="fas fa-search"></i> Main Findings</h4>
                <ul>
                    <li><strong>Terminology Differences:</strong> Positive sentiment texts frequently use scientific and action-oriented terms like "emissions," "climate," and "pollution," focusing on the environmental impact.</li>
                    <li><strong>Skepticism Markers:</strong> The term "propaganda" appears as the most frequent word in negative sentiment texts, suggesting skepticism about climate change information.</li>
                    <li><strong>Shared Vocabulary:</strong> Interestingly, both positive and negative classes share terms like "nuclear," "people," and "air," but likely use these in different contexts.</li>
                    <li><strong>Technical vs. Political Framing:</strong> Positive texts appear to use more technical environmental language, while negative texts contain more politically charged terminology.</li>
                </ul>
                
                <h4><i class="fas fa-file-alt"></i> Implications</h4>
                <p>These findings suggest that sentiment around climate change follows distinctive linguistic patterns that machine learning models can effectively detect. The neural network's high accuracy (99.67%) demonstrates that these patterns are consistent enough to enable reliable automated sentiment classification.</p>
                <p>The difference between my results with Support Vector Machines (SVM) and this feed-forward neural network is that the neural network is able to capture more complex patterns in the data, as well as my performace in preparing the data for supervised machine learning.</p>
            </div>
        </section>
    </div>
</body>
</html>
