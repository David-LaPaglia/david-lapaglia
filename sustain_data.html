<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="style.css" rel="stylesheet">
    <title>David LaPaglia</title>
</head>
<body>
    

    <nav id="nav">
        <ul>
            <li><a href="index.html">HOME</a></li>
            <li><a href="about.html">ABOUT ME</a></li>
            <li class="dropdown">
                <a href="projects.html">PROJECTS</a>
                <ul class="dropdown-content">
                    <li><a href="projects.html">ALL PROJECTS</a></li>
                    <li class="dropdown">
                        <a href="textmining.html">TEXT MINING PROJECTS</a>
                        <ul class="dropdown-content secondary-dropdown">
                            <li><a href="sustainabilityanalysis.html">SUSTAINABILITY ANALYSIS</a></li>
                            <li><a href="neural_networks.html">NEURAL NETWORKS</a></li>
                            <li><a href="sustain_intro.html">INTRODUCTION</a></li>
                            <li><a href="sustain_data.html">DATA</a></li>
                            <li><a href="textmining_conclusion.html" class="active">CONCLUSIONS</a></li>
                        </ul>
                    </li>
                </ul>
            </li>
            <li><a href="resume.html">MY RESUME</a></li>
        </ul>
    </nav>

<h1> .</h1>
<div class="content">
    <h3>Is climate change a threat to our future?</h3>
    <h4>Data</h4>
    <h4>Module 1</h4>
    <p>To gather data, I used 'three' sort of sources. The reddit api wrapper, or PRAW, to scrape data from reddit, NEWSAPI to scrape data from news sources, and BeautifulSoup for detailed article content extraction.</p>

    <h4>News API Data Collection</h4>
    <p>The News API provided a structured way to gather climate-related news articles. Here's how I approached it:</p>
    <ul>
        <li>Used endpoints like <code>https://newsapi.org/v2/everything?q=bitcoin</code> with appropriate API keys</li>
        <li>Collected raw JSON data including article metadata, URLs, and preview content</li>
        <li>Implemented BeautifulSoup to extract full article content from the provided URLs</li>
    </ul>

    <h4>Data Processing Pipeline</h4>
    <p>The data collection and processing pipeline involved several sophisticated steps to ensure data quality and usability:</p>

    <h5>1. Initial Data Collection</h5>
    <p>The raw data from multiple sources required different handling approaches:</p>
    <div class="data-comparison">
        <div class="before-data">
            <h5>NewsAPI Call:</h5>
            <pre>
topic = 'air pollution'
URLPost = {
    'apiKey':n_creds['api_key'],
    'source': 'bbc-news',
    'pageSize': 100,
    'totalRequests': 100,
    'q':'air pollution'}
req = requests.get('https://newsapi.org/v2/everything', params=URLPost)
            </pre>
            <p>After this, I get a JSON sructure dataset with the following format:</p>
            <h5>Before Processing:</h5>
            <p>Raw JSON data included mixed content types, null values, and nested structures like:</p>
            <pre>
{
"status": "ok",
"totalResults": 14185,
"articles": [
    {
        "source": {
            "id": null,
            "name": "Yahoo Entertainment"
        },
        "author": "Anna Washenko",
        "title": "Climate change increased the odds of Los Angeles' devastating fires...",
        "description": "As Los Angeles reels from the loss of lives and homes to the...",
        "url": "https://consent.yahoo.com/v2/...d7326db44fc0",
        "urlToImage": null,
        "publishedAt": "2025-01-29T21:16:52Z",
        "content": "If you click 'Accept all', we and our partners, ...[+703 chars]"
    },
            </pre>
        </div>
        <div class="after-data">
            <h5>After Processing:</h5>
            <p>Cleaned and vectorized data with standardized format:</p>
            <pre>
                        ability  abuses  ac  academic  accelerating  according  \
Android_Authority            0       0   8         0             0          0   
The_Times_of_India           0       0   0         0             0          0   
Nist                         0       0   0         0             0          0   
Vox                          0       0   0         0             0          0   
Naturalnews                  0       0   0         0             0          0   
            </pre>
            <p>This is a snippet of the same corpus from the JSON data above. The processing pipeline follows these steps:</p>
            <h3>If you want to view all the data and code for the project -> Check out my github: <a href="https://github.com/David-LaPaglia/Projects/tree/main/text-mining/text_mining-1 ">Module 1 Data</a></h3>
            <ol>
                <li>Data Collection: JSON format data gathered from news sources</li>
                <li>Text Extraction: Programmatically converted into a corpus of .txt files, organized by news source</li>
                <li>Preprocessing:
                    <ul>
                        <li>Removal of HTML tags and special characters</li>
                        <li>Tokenization of text into individual words</li>
                        <li>Removal of stop words and rare terms</li>
                        <li>Lemmatization to reduce words to their base form</li>
                    </ul>
                </li>
                <li>Vectorization: Implementation of both Count Vectorizer and TF-IDF Vectorizer to create sparse matrices</li>
            </ol>
            
            <p>The TF-IDF (Term Frequency-Inverse Document Frequency) vectorizer was particularly crucial as it:</p>
            <ul>
                <li>Normalizes word frequencies across documents</li>
                <li>Assigns higher weights to terms that are unique to specific documents</li>
                <li>Reduces the impact of common words that appear across many documents</li>
                <li>Creates a more nuanced representation of document importance</li>
            </ul>
        </div>
    </div>

    <h4>Reddit Data Collection</h4>
    <p>For Reddit data collection, I utilized PRAW (Python Reddit API Wrapper). An example of the data structure is as follows:</p>
    <img src="images/unclean_data1.png" class="project-image">
    <p>When using PRAW I created a seperate python file to contain a majority of my python reddit helper functions. That 
        is what is called, pythonHelperFunctions.py in this case. This file was imported into my main python file.
        There were two majors functions I wrote to gather data from reddit, one to get comments from a specific subreddit, useful for 
        gathering labeled data soley from a specific subreddit. The other fucntion was useful to 
        search through all the subreddits based on specicif search term - this is what I used for the majority later 
        on in the project.
    </p>
    <img src='images/function1.png' class="project-image">
    <p>and</p>
    <img src='images/function2.png' class="project-image">
    <p>It was important to account for no text because in cases of empty returns it would break my code when I was trying to dump no data into a json.</p>
    <p>The JSON data structures from both News API and Reddit were semi-structured, presenting unique challenges in data cleaning and standardization. Each source required specific handling:</p>
    <ul>
        <li>News API data needed URL validation and content extraction</li>
        <li>Reddit data required handling of deleted comments and varying content lengths</li>
        <li>Both sources needed consistent date formatting and text normalization</li>
    </ul>

    <h4>Data Quality Assurance and Challenges</h4>
    <p>To ensure data quality and replicability, I implemented several validation steps and addressed various challenges:</p>
    
    <h5>Data Validation Steps:</h5>
    <ul>
        <li>Automated error handling for API rate limits and connection issues</li>
        <li>Data completeness checks before JSON storage</li>
        <li>Consistent text encoding and special character handling</li>
        <li>Detailed logging of data collection and processing steps</li>
        <li>Validation of data for integrity and consistency</li>
        <li>Filtering of non-Latin alphabet characters</li>
    </ul>

    <h5>Technical Challenges Addressed:</h5>
    <ul>
        <li>API Rate Limiting: Implemented exponential backoff strategy for API requests</li>
        <li>...[int+] Characters Remaining | Much of the API didn't return the stories of the query</li>
        <li>Data Consistency:Developed robust error handling for missing or malformed data</li>
        <li>Text Encoding: Standardized UTF-8 encoding across all sources</li>
        <li>Memory Management: Implemented batch processing for large datasets</li>
    </ul>

    <h5>Data Visualization and Analysis:</h5>
    <p>To better understand the collected data:</p>
    <ul>
        <li>Word Clouds: Visual representation of term frequency across sources</li>
        <img src="images/wordcloud.png" class="project-image">
        <li>Dataframes: Data exploration and analysis</li>
    </ul>

    <h5>Web Scraped TFIDF:</h5>
    <p>After I gathered URLs from the NEWSAPI I Programmatically scraped content from the sites - then I made a TFIDF sparse matrix out of that newly created corpus.</p>
    <img src="images/tfidf.png" class="project-image">

    <h5>NewsAPI Data Deliverable</h5>
    <p>I did two queries with the News api to get the data that I collected. This then turned into two seperate dataframes that were vecotrized. One datafram is of vectorized data from the query "air pollution" and the other the term "climate change".</p>
    <h6>Climate Change Data</h6>
    <img src="images/news_api_climate_change.png" class="project-image">
    <h6>Air Pollution Data</h6>
    <img src="images/news_api_air_pollution.png" class="project-image">

    <h3> Key Data Processing Function</h3>
    <p>This function would be plugged right into the vecotorizer objects to systemattically clean and process the data!</p>
    <img src="images/data_clean_function.png" class="project-image">
    <p>This was implemented into the vectorizer objects from scikit learn to process the data before it vectorized the words. I needed stemming and lemmitization to occur and found it conveinvent that we can implement it right into the object.</p>

    <h5>Next Steps</h5>
    <p>With the data collection and processing pipeline in place, stay tuned for the next steps -> topics include: Exploratory Data Analysis, Sentiment Analysis, and even experimenting with some basic ML models to contextualize this data. We've collected this data, now we have to synthesize them.</p>    
</div>

    <footer>
        <div class="container">
            <div class="footer-content">
                <div class="social-links">
                    <a href="https://github.com/David-LaPaglia/Projects" target="_blank" class="social-icon">
                        <i class="fab fa-github"></i>
                    </a>
                    <a href="https://www.linkedin.com/in/david-lapaglia/" target="_blank" class="social-icon">
                        <i class="fab fa-linkedin"></i>
                    </a>
                    <a href="mailto:david.lapaglia@colorado.edu" class="social-icon">
                        <i class="fas fa-envelope"></i>
                    </a>
                </div>
                <p>&copy; 2025 David LaPaglia. All rights reserved.</p>
            </div>
        </div>
    </footer>

    <script src="script.js"></script>
</body>
</html>